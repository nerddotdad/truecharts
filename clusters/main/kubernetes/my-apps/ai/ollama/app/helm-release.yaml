apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ai
spec:
  interval: 15m
  chart:
    spec:
      chart: ollama
      version: 8.15.15
      sourceRef:
        kind: HelmRepository
        name: truecharts
        namespace: flux-system
      interval: 15m
  timeout: 5m
  maxHistory: 3
  driftDetection:
    mode: warn
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    securityContext:
      container:
        runAsNonRoot: false
        readOnlyRootFilesystem: false
        runAsUser: 0
        runAsGroup: 0

    podOptions:
      nodeSelector:
        kubernetes.io/hostname: k8s-worker-1
      runtimeClassName: "nvidia"

    resources:
        requests:
          nvidia.com/gpu: 1
          memory: 1Gi  # Actual usage is ~272Mi, 1Gi provides headroom
        limits:
          nvidia.com/gpu: 1
          memory: 2Gi  # Reduced from 4Gi - actual usage is ~272Mi, 2Gi is plenty
          # This frees up 2Gi of memory limit allocation for the node

    workload:
      main:
        type: Deployment
        podSpec:
          runtimeClassName: "nvidia"
          containers:
            main:
              env:
                OLLAMA_HOST: "0.0.0.0:11434"
                OLLAMA_ORIGINS: "*"
              resources:
                limits:
                  memory: 2Gi  # Reduced from 4Gi - actual usage is ~272Mi
                requests:
                  memory: 1Gi  # Reduced from 2Gi - actual usage is ~272Mi
      ui:
        type: Deployment
        podSpec:
          nodeSelector:
            kubernetes.io/hostname: k8s-worker-1
          # Tolerate gpu-workloads taint on worker node
          tolerations:
            - key: gpu-workloads
              operator: Equal
              value: "required"
              effect: NoSchedule

    persistence:
      config:
        enabled: true
        type: nfs
        mountPath: "/root/.ollama"
        path: ${NFS_APPS}/ollama/config
        server: ${NAS_IP}
        targetSelector:
          main:
            main:
              mountPath: "/root/.ollama"
      data:
        enabled: true
        type: nfs
        mountPath: "/app/backend/data"
        path: ${NFS_APPS}/ollama/data
        server: ${NAS_IP}
        targetSelector:
          ui:
            ui:
              mountPath: "/app/backend/data"

    ingress:
      main:
        primary: true
        enabled: true
        ingressClassName: external
        hosts:
          - host: ollama.${DOMAIN_0}
            paths:
              - path: /
                pathType: Prefix
        integrations:
          traefik:
            enabled: false
          certManager:
            enabled: true
            certificateIssuer: "domain-0-le-prod"
      api:
        enabled: true
        ingressClassName: external
        hosts:
          - host: ollama-api.${DOMAIN_0}
            paths:
              - path: /
                pathType: Prefix
        integrations:
          traefik:
            enabled: false
          certManager:
            enabled: true
            certificateIssuer: "domain-0-le-prod"
