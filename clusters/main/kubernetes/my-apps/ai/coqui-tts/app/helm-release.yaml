apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: coqui-tts
  namespace: ai
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 15.24.38
      sourceRef:
        kind: HelmRepository
        name: truecharts
        namespace: flux-system
      interval: 15m
  values:
    image:
      repository: ghcr.io/coqui-ai/tts
      pullPolicy: IfNotPresent
      tag: "latest"  # GPU version - pre-pull on node to avoid timeout
    
    securityContext:
      container:
        runAsNonRoot: false
        readOnlyRootFilesystem: false
        runAsUser: 0
        runAsGroup: 0

    podOptions:
      nodeSelector:
        kubernetes.io/hostname: k8s-worker-1
      runtimeClassName: "nvidia"
      tolerations:
        - key: gpu-workloads
          operator: Equal
          value: "required"
          effect: NoSchedule

    workload:
      main:
        podSpec:
          runtimeClassName: "nvidia"
          containers:
            main:
              enabled: true
              probes:
                startup:
                  enabled: true
                  type: http
                  port: 5002
                  path: /  # Flask web interface root endpoint
                  initialDelaySeconds: 30  # Initial delay before first probe
                  periodSeconds: 10
                  failureThreshold: 60  # Allow up to 10 minutes for model download (Bark models are large)
                liveness:
                  enabled: true
                  type: http
                  port: 5002
                  path: /  # Flask web interface root endpoint
                  periodSeconds: 30
                  failureThreshold: 3
                readiness:
                  enabled: true
                  type: http
                  port: 5002
                  path: /  # Flask web interface root endpoint
                  initialDelaySeconds: 30  # Wait before starting readiness checks
                  periodSeconds: 10
                  failureThreshold: 60  # Allow up to 10 minutes for model download
              env:
                # Coqui TTS configuration - using Bark model
                # Bark is multilingual and supports voice cloning, but requires GPU (already enabled)
                # Note: Bark is slower than VITS but offers voice cloning capabilities
                TTS_MODEL: "tts_models/multilingual/multi-dataset/bark"
                TTS_VOCAB_MODEL: ""  # Bark doesn't use a separate vocoder
              resources:
                requests:
                  memory: "4Gi"  # Increased for Bark model
                  cpu: "1000m"
                  nvidia.com/gpu: 1
                limits:
                  memory: "8Gi"  # Increased for Bark model
                  cpu: "2000m"
                  nvidia.com/gpu: 1
              command:
                - "/bin/bash"
                - "-c"
                - |
                  # Start custom TTS server with Bark model
                  # tts-server doesn't fully support Bark, so we use TTS API directly with Flask
                  python3 << 'EOF'
                  from flask import Flask, request, send_file
                  from TTS.api import TTS
                  import os
                  import tempfile
                  
                  app = Flask(__name__)
                  
                  # Initialize Bark model
                  print("Loading Bark model...")
                  try:
                      import torch
                      # Try loading Bark model
                      # If there's a corrupted cache, it will re-download
                      tts = TTS("tts_models/multilingual/multi-dataset/bark")
                      # Use newer API to move to GPU
                      device = "cuda" if torch.cuda.is_available() else "cpu"
                      tts.to(device)
                      print(f"Bark model loaded successfully on {device}!")
                  except Exception as e:
                      print(f"Error loading Bark model: {e}")
                      print("This might be due to corrupted model files. The model will be re-downloaded on next attempt.")
                      raise
                  
                  @app.route('/')
                  def index():
                      return "Coqui TTS Server with Bark - API available at /api/tts"
                  
                  @app.route('/api/tts', methods=['POST'])
                  def synthesize():
                      try:
                          # Get text from form-data or JSON
                          if request.is_json:
                              text = request.json.get('text', '')
                          else:
                              text = request.form.get('text', '')
                          
                          if not text:
                              return {"error": "text parameter required"}, 400
                          
                          # Generate audio to temp file
                          with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                              output_path = tmp_file.name
                          
                          tts.tts_to_file(text=text, file_path=output_path)
                          
                          # Return audio file
                          return send_file(
                              output_path,
                              mimetype='audio/wav',
                              as_attachment=False,
                              download_name='output.wav'
                          )
                      except Exception as e:
                          import traceback
                          error_msg = f"Error: {str(e)}\n{traceback.format_exc()}"
                          print(error_msg)
                          return {"error": str(e)}, 500
                  
                  if __name__ == '__main__':
                      app.run(host='0.0.0.0', port=5002, debug=False)
                  EOF

    persistence:
      models:
        enabled: true
        type: nfs
        mountPath: "/root/.local/share/tts"
        path: ${NFS_APP_CONFIGS}/coqui-tts/models
        server: ${NAS_IP}

    service:
      main:
        enabled: true
        ports:
          main:
            enabled: true
            protocol: tcp
            port: 5002
            targetPort: 5002

    ingress:
      main:
        enabled: true
        ingressClassName: internal
        hosts:
          - host: coqui-tts.${DOMAIN_0}
            paths:
              - path: /
                pathType: Prefix
        integrations:
          traefik:
            enabled: false
          certManager:
            enabled: true
            certificateIssuer: "domain-0-le-prod"

