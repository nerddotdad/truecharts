apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: coqui-tts
  namespace: ai
spec:
  interval: 15m
  chart:
    spec:
      chart: app-template
      version: 15.24.38
      sourceRef:
        kind: HelmRepository
        name: truecharts
        namespace: flux-system
      interval: 15m
  values:
    image:
      repository: ghcr.io/coqui-ai/tts
      pullPolicy: IfNotPresent
      tag: "latest"  # GPU version - pre-pull on node to avoid timeout
    
    securityContext:
      container:
        runAsNonRoot: false
        readOnlyRootFilesystem: false
        runAsUser: 0
        runAsGroup: 0

    podOptions:
      nodeSelector:
        kubernetes.io/hostname: k8s-worker-1
      runtimeClassName: "nvidia"
      tolerations:
        - key: gpu-workloads
          operator: Equal
          value: "required"
          effect: NoSchedule

    workload:
      main:
        podSpec:
          runtimeClassName: "nvidia"
          containers:
            main:
              enabled: true
              probes:
                startup:
                  enabled: true
                  type: http
                  port: 5002
                  path: /  # Flask root endpoint - returns 503 until model loads
                  initialDelaySeconds: 10  # Start checking quickly
                  periodSeconds: 10
                  failureThreshold: 120  # Allow up to 20 minutes for model download
                liveness:
                  enabled: true
                  type: http
                  port: 5002
                  path: /  # Flask root endpoint
                  periodSeconds: 30
                  failureThreshold: 3
                readiness:
                  enabled: true
                  type: http
                  port: 5002
                  path: /ready  # Returns 200 only when model is loaded
                  initialDelaySeconds: 10  # Start checking quickly
                  periodSeconds: 10
                  failureThreshold: 120  # Allow up to 20 minutes for model download
              resources:
                requests:
                  memory: "4Gi"  # Increased for Bark model
                  cpu: "1000m"
                  nvidia.com/gpu: 1
                limits:
                  memory: "8Gi"  # Increased for Bark model
                  cpu: "2000m"
                  nvidia.com/gpu: 1
              command:
                - "/bin/bash"
                - "-c"
                - |
                  # tts-server doesn't support Bark, so use TTS API directly
                  python3 << 'EOF'
                  from flask import Flask, request, send_file
                  from TTS.api import TTS
                  import torch
                  import tempfile
                  import os
                  import shutil
                  import threading
                  
                  app = Flask(__name__)
                  tts = None
                  loading_error = None
                  
                  def check_file_integrity(filepath):
                      """Check if a file is corrupted (HTML error page instead of binary)"""
                      if not os.path.exists(filepath):
                          return False
                      try:
                          with open(filepath, 'rb') as f:
                              first_bytes = f.read(100)
                              # Check if it starts with HTML tags (error page)
                              if first_bytes.startswith(b'<') or b'<html' in first_bytes.lower() or b'<!doctype' in first_bytes.lower():
                                  print(f"Warning: {filepath} appears to be HTML (corrupted download)")
                                  return False
                              # Check if file is suspiciously small (likely incomplete)
                              file_size = os.path.getsize(filepath)
                              if file_size < 1024:  # Less than 1KB is suspicious for model files
                                  print(f"Warning: {filepath} is suspiciously small ({file_size} bytes)")
                                  return False
                          return True
                      except Exception as e:
                          print(f"Error checking {filepath}: {e}")
                          return False
                  
                  def clear_all_caches():
                      """Clear all model-related caches"""
                      cache_dirs = [
                          "/root/.local/share/tts/tts_models--multilingual--multi-dataset--bark",
                          "/root/.cache/torch/hub/checkpoints"
                      ]
                      for cache_dir in cache_dirs:
                          if os.path.exists(cache_dir):
                              print(f"Clearing cache: {cache_dir}")
                              try:
                                  if os.path.isdir(cache_dir):
                                      shutil.rmtree(cache_dir)
                                  else:
                                      os.remove(cache_dir)
                              except Exception as e:
                                  print(f"Warning: Could not clear {cache_dir}: {e}")
                  
                  def load_model():
                      global tts, loading_error
                      try:
                          model_dir = "/root/.local/share/tts/tts_models--multilingual--multi-dataset--bark"
                          
                          # Check if model directory exists but is incomplete
                          if os.path.exists(model_dir) and not os.path.exists(os.path.join(model_dir, "config.json")):
                              print("Incomplete model download detected. Clearing cache...")
                              clear_all_caches()
                          
                          # Try loading with retry on corruption
                          print("Loading Bark model...")
                          max_attempts = 3
                          for attempt in range(max_attempts):
                              try:
                                  tts = TTS("tts_models/multilingual/multi-dataset/bark")
                                  tts.to("cuda" if torch.cuda.is_available() else "cpu")
                                  print("Bark model loaded successfully!")
                                  break
                              except Exception as e:
                                  error_str = str(e)
                                  if "invalid load key" in error_str or "UnpicklingError" in error_str:
                                      print(f"Corrupted model files detected (attempt {attempt + 1}/{max_attempts}). Clearing all caches...")
                                      clear_all_caches()
                                      if attempt == max_attempts - 1:
                                          loading_error = f"Failed after {max_attempts} attempts: {error_str}"
                                          raise
                                      print(f"Waiting 5 seconds before retry...")
                                      import time
                                      time.sleep(5)
                                  else:
                                      loading_error = str(e)
                                      raise
                      except Exception as e:
                          loading_error = str(e)
                          print(f"Failed to load model: {e}")
                  
                  # Start model loading in background
                  threading.Thread(target=load_model, daemon=True).start()
                  
                  @app.route('/')
                  def index():
                      # Always return 200 - server is running (for startup probe)
                      if tts:
                          return "Coqui TTS Server with Bark - Ready", 200
                      elif loading_error:
                          return f"Coqui TTS Server - Error: {loading_error}", 200
                      else:
                          return "Coqui TTS Server - Loading model...", 200
                  
                  @app.route('/ready')
                  def ready():
                      # Return 200 only when model is loaded (for readiness probe)
                      if tts:
                          return "Ready", 200
                      elif loading_error:
                          return f"Error: {loading_error}", 503
                      else:
                          return "Loading...", 503
                  
                  @app.route('/api/tts', methods=['POST'])
                  def synthesize():
                      if not tts:
                          return {"error": "Model still loading"}, 503
                      
                      text = request.form.get('text') or (request.json or {}).get('text', '')
                      if not text:
                          return {"error": "text parameter required"}, 400
                      
                      with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as f:
                          tts.tts_to_file(text=text, file_path=f.name)
                          return send_file(f.name, mimetype='audio/wav')
                  
                  app.run(host='0.0.0.0', port=5002, debug=False)
                  EOF

    persistence:
      models:
        enabled: true
        type: nfs
        mountPath: "/root/.local/share/tts"
        path: ${NFS_APP_CONFIGS}/coqui-tts/models
        server: ${NAS_IP}

    service:
      main:
        enabled: true
        ports:
          main:
            enabled: true
            protocol: tcp
            port: 5002
            targetPort: 5002

    ingress:
      main:
        enabled: true
        ingressClassName: internal
        hosts:
          - host: coqui-tts.${DOMAIN_0}
            paths:
              - path: /
                pathType: Prefix
        integrations:
          traefik:
            enabled: false
          certManager:
            enabled: true
            certificateIssuer: "domain-0-le-prod"

